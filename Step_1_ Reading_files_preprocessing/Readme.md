# Step 1:-

Write a python script to perform the following data preparation activities:
1. Tokenize the corpus
2. Remove the following special characters: !"#$%&()*+/:;<=>@[\\]^`{|}~\t\n
3. Create two versions of your dataset: (1) with stopwords and (2) without stopwords.
Stopword lists are available online.
4. Randomly split your data into training (80%), validation (10%) and test (10%) sets.

# Exceution Details:-

I have only placed the .py files in GitHub. The MSCI_641_Assignement_1.py file contains all the execution and the main.py is the file to be executed which has the driver of MSCI_641_Assignement_1.py.



